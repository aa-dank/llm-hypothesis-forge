{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87e3d24d",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) Perplexity Scoring Demo\n",
    "\n",
    "This notebook demonstrates how to use the RAG pipeline to generate perplexity scores for research paper abstracts. We'll walk through the entire process:\n",
    "\n",
    "1. **Loading sample research paper abstracts** - We'll start with two versions of an abstract (original and modified)\n",
    "2. **Creating masked abstracts** - Identifying and masking differences between the two abstracts\n",
    "3. **Generating queries for research services** - Creating semantic search queries based on the masked abstract\n",
    "4. **Retrieving relevant papers** - Sending queries to research paper services\n",
    "5. **Culling citation relationships** - Ensuring we don't include papers that cite our test paper\n",
    "6. **Generating RAG context** - Creating context from retrieved papers\n",
    "7. **Calculating perplexity scores** - Comparing perplexity scores to identify the \"real\" abstract\n",
    "\n",
    "Note: This notebook needs to be run from the root project directory. (Not from the `notebooks` directory)\n",
    "\n",
    "Let's begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89e8574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "from pprint import pprint\n",
    "import datetime\n",
    "import zlib\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "\n",
    "# Import project-specific modules\n",
    "from data.db import get_db_session, close_db_session\n",
    "from data.models import ResearchPaper, PerplexityScoreEvent\n",
    "from rag_orchestration.utils import (\n",
    "    mask_abstract_differences,\n",
    "    generate_service_queries,\n",
    "    dispatch_queries,\n",
    "    cull_citing_papers,\n",
    "    generate_rag_context,\n",
    "    assemble_test_abstract_prompt\n",
    ")\n",
    "from prompts_library.rag import service_query_creation_template\n",
    "from services.llm_services import BasicOpenAI, TogetherClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cf608e",
   "metadata": {},
   "source": [
    "## Database Connection and Sample Paper\n",
    "\n",
    "Instead of fetching data from the database, we'll create a toy sample paper with original and modified abstracts to demonstrate the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7215c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = get_db_session()\n",
    "\n",
    "# Toy sample paper but noramlly would be loaded from the database\n",
    "sample_paper = ResearchPaper(\n",
    "    id=999999,  # Use a dummy ID that won't conflict with real records\n",
    "    doi=\"10.1234/demo.12345\",\n",
    "    title=\"The Effect of Neural Oscillations on Cognitive Performance in Laboratory Settings\",\n",
    "    authors=\"Smith, J., Johnson, M., & Williams, P.\",\n",
    "    date=\"2023-04-15\",\n",
    "    abstract=\"\"\"Neural oscillations have been implicated in numerous cognitive processes, including attention, memory, and executive function. In this study, we examined the relationship between different frequency bands of neural oscillations and cognitive performance in a controlled laboratory setting. Electroencephalography (EEG) data was collected from 45 healthy participants while they performed a battery of cognitive tasks. Results showed that theta band (4-8 Hz) power positively correlated with working memory performance, while alpha band (8-12 Hz) power was inversely related to attentional control. Beta oscillations (13-30 Hz) were found to predict performance on tasks requiring inhibitory control. These findings suggest that specific neural oscillation patterns serve as electrophysiological signatures of distinct cognitive processes. Our results contribute to the growing understanding of the neural mechanisms underlying cognitive function and may have implications for interventions targeting cognitive enhancement.\"\"\",\n",
    "    gpt4_incorrect_abstract=\"\"\"Neural oscillations have been implicated in numerous cognitive processes, including attention, memory, and executive function. In this study, we examined the relationship between different frequency bands of neural oscillations and cognitive performance in a controlled laboratory setting. Electroencephalography (EEG) data was collected from 45 healthy participants while they performed a battery of cognitive tasks. Results showed that theta band (4-8 Hz) power negatively correlated with working memory performance, while alpha band (8-12 Hz) power was directly related to attentional control. Gamma oscillations (30-100 Hz) were found to predict performance on tasks requiring inhibitory control. These findings suggest that specific neural oscillation patterns serve as electrophysiological signatures of distinct cognitive processes. Our results contribute to the growing understanding of the neural mechanisms underlying cognitive function and may have implications for interventions targeting cognitive enhancement.\"\"\",\n",
    "    category=\"neuroscience\",\n",
    "    license=\"CC-BY-4.0\",\n",
    "    published_journal=\"Journal of Cognitive Neuroscience\"\n",
    ")\n",
    "\n",
    "print(f\"Created test paper: {sample_paper.title}\")\n",
    "print(f\"DOI: {sample_paper.doi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae22f0e",
   "metadata": {},
   "source": [
    "## Sample Paper Abstracts\n",
    "\n",
    "Let's examine both the original and modified (GPT-4 incorrect) versions of the abstract. \n",
    "\n",
    "The original abstract represents the real content of the paper, while the modified abstract contains subtle but scientifically significant changes that alter the meaning or claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e693f8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ORIGINAL ABSTRACT ===\")\n",
    "print(sample_paper.abstract)\n",
    "print(\"\\n=== GPT-4 MODIFIED ABSTRACT ===\")\n",
    "print(sample_paper.gpt4_incorrect_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7471b755",
   "metadata": {},
   "source": [
    "## Creating a Masked Abstract\n",
    "\n",
    "Now we need to identify the differences between the original and modified abstracts. The `mask_abstract_differences` function compares both versions and replaces different segments with placeholders using double brackets (e.g., `[[DIFF]]`).\n",
    "\n",
    "This masking process is crucial as it helps us create search queries that won't be biased towards either version of the abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7519d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a masked abstract by identifying and bracketing differences\n",
    "masked_abstract = mask_abstract_differences(\n",
    "    abstract_a=sample_paper.abstract,\n",
    "    abstract_b=sample_paper.gpt4_incorrect_abstract\n",
    ")\n",
    "\n",
    "print(\"=== MASKED ABSTRACT ===\")\n",
    "print(masked_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5dd01e",
   "metadata": {},
   "source": [
    "## Generating Service Queries\n",
    "\n",
    "Next, we'll use the masked abstract to generate search queries for various academic research services. We use an LLM (in this case, OpenAI's model) to generate appropriate search queries based on the content of the masked abstract.\n",
    "\n",
    "The queries will target services like ArXiv, bioRxiv, PubMed, etc. while avoiding specific terms that might bias the search toward either the original or modified versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ffb336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client for query generation\n",
    "llm_client = BasicOpenAI()\n",
    "\n",
    "# Generate service queries based on the masked abstract\n",
    "query_plan = generate_service_queries(\n",
    "    masked_abstract=masked_abstract,\n",
    "    llm_client=llm_client,\n",
    "    template_str=service_query_creation_template\n",
    ")\n",
    "\n",
    "print(\"=== GENERATED SERVICE QUERIES ===\")\n",
    "pprint(query_plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b78c903",
   "metadata": {},
   "source": [
    "## Dispatching Queries to Services\n",
    "\n",
    "Now we'll send the generated queries to various research services to retrieve relevant papers. The `dispatch_queries` function handles communication with these services and returns a list of research papers that match our queries.\n",
    "\n",
    "Each service may return papers in different formats, but our function standardizes them into `ResearchPaper` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a1666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dispatch queries to services\n",
    "results = dispatch_queries(query_plan, max_results_per_query=5)\n",
    "\n",
    "# Flatten the results into a single list of papers\n",
    "candidate_papers = [p for papers in results.values() for p in papers]\n",
    "\n",
    "# Display the number of papers returned\n",
    "print(f\"Retrieved {len(candidate_papers)} candidate papers from all services\")\n",
    "print(\"\\nSample of retrieved papers:\")\n",
    "for i, paper in enumerate(candidate_papers[:3], 1):\n",
    "    print(f\"\\n{i}. {paper.title}\")\n",
    "    print(f\"   DOI: {paper.doi}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8901cac4",
   "metadata": {},
   "source": [
    "## Culling Papers with Citation Relationships\n",
    "\n",
    "To prevent \"data leakage\" in our RAG context, we need to remove any papers that cite our test paper. This step is crucial for maintaining the integrity of the perplexity scoring.\n",
    "\n",
    "If we included papers that cite our test paper, the RAG context might contain information directly derived from the test paper, making it too easy to distinguish between the original and modified abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc8e014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove papers that cite our test paper\n",
    "culled_papers = cull_citing_papers(\n",
    "    candidate_papers, \n",
    "    test_doi=sample_paper.doi, \n",
    "    include_opencitations=True\n",
    ")\n",
    "\n",
    "print(f\"Original candidate papers: {len(candidate_papers)}\")\n",
    "print(f\"After culling papers that cite test paper: {len(culled_papers)}\")\n",
    "print(f\"Removed {len(candidate_papers) - len(culled_papers)} citing papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7feea58",
   "metadata": {},
   "source": [
    "## Fetching Full Text and Generating RAG Context\n",
    "\n",
    "Next, we'll fetch the full text of the remaining papers and generate a RAG context. This involves:\n",
    "\n",
    "1. Ensuring full-text chunks and embeddings are stored in the database\n",
    "2. Performing vector similarity search to find relevant chunks\n",
    "3. Assembling these chunks into a coherent context\n",
    "\n",
    "If vector search fails to return sufficient data, we'll use a fallback method of concatenating full texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ae9e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional required function\n",
    "from rag_orchestration.utils import get_or_fetch_research_paper_text\n",
    "\n",
    "# Ensure full-text chunks and embeddings are stored\n",
    "print(\"Fetching full text for culled papers...\")\n",
    "for i, paper in enumerate(culled_papers[:5], 1):  # Limit to 5 for demo purposes\n",
    "    try:\n",
    "        text = get_or_fetch_research_paper_text(paper, session)\n",
    "        print(f\"Paper {i}/{min(5, len(culled_papers))}: Retrieved {len(text) if text else 0} chars\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching text for DOI {paper.doi}: {str(e)}\")\n",
    "\n",
    "# Generate RAG context using vector similarity search\n",
    "print(\"\\nGenerating RAG context...\")\n",
    "rag_context, chunks_retrieved = generate_rag_context(\n",
    "    session, \n",
    "    masked_abstract, \n",
    "    k=15,  # Number of chunks to retrieve\n",
    "    return_chunk_count=True\n",
    ")\n",
    "\n",
    "print(f\"Retrieved {chunks_retrieved} text chunks for RAG context\")\n",
    "print(f\"Generated RAG context with {len(rag_context)} characters\")\n",
    "\n",
    "# Truncate if necessary to a reasonable size for demonstration\n",
    "rag_size_chars = 10000\n",
    "if len(rag_context) > rag_size_chars:\n",
    "    print(f\"Truncating RAG context from {len(rag_context)} to {rag_size_chars} chars\")\n",
    "    rag_context = rag_context[:rag_size_chars]\n",
    "    # Snap to a clean break\n",
    "    for sep in (\". \", \".\\n\", \"\\n\\n\", \"\\n\"):\n",
    "        pos = rag_context.rfind(sep, max(0, rag_size_chars - 200))\n",
    "        if pos != -1:\n",
    "            rag_context = rag_context[: pos + len(sep)]\n",
    "            break\n",
    "\n",
    "print(f\"Final RAG context: {len(rag_context)} chars\")\n",
    "print(\"\\nSample of RAG context (first 500 chars):\")\n",
    "print(rag_context[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c728ce9a",
   "metadata": {},
   "source": [
    "## Calculating Perplexity Scores\n",
    "\n",
    "Now we'll calculate perplexity scores for both the original and modified abstracts using the RAG context. The perplexity score measures how \"surprised\" the language model is by the text. Lower perplexity indicates text that seems more natural or expected to the model.\n",
    "\n",
    "In our case, we expect the original abstract to have a lower perplexity score than the modified one, as the RAG context should provide information that corroborates the claims in the original abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185ef4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Together.ai client for perplexity scoring\n",
    "together_model = \"meta-llama/Llama-3.3-70B-Instruct-Turbo\"\n",
    "tc = TogetherClient(model=together_model)\n",
    "\n",
    "# Helper function to calculate and display perplexity\n",
    "def calculate_perplexity(abstract_text, abstract_source):\n",
    "    # Assemble the full prompt with RAG context and abstract\n",
    "    full_prompt = assemble_test_abstract_prompt(rag_context.strip(), abstract_text.strip())\n",
    "    \n",
    "    print(f\"\\n=== {abstract_source.upper()} ABSTRACT ===\")\n",
    "    print(f\"Prompt size: {len(full_prompt)} chars\")\n",
    "    \n",
    "    # Calculate perplexity and zlib ratio\n",
    "    perplexity = tc.perplexity_score(full_prompt)\n",
    "    zlib_size = len(zlib.compress(full_prompt.encode(\"utf-8\")))\n",
    "    zlib_ratio = perplexity / zlib_size if zlib_size else None\n",
    "    \n",
    "    print(f\"Perplexity score: {perplexity:.6f}\")\n",
    "    print(f\"Zlib compression size: {zlib_size}\")\n",
    "    print(f\"Zlib-perplexity ratio: {zlib_ratio:.8f}\")\n",
    "    \n",
    "    return perplexity, zlib_size, zlib_ratio\n",
    "\n",
    "# Calculate perplexity for both abstracts\n",
    "print(f\"Calculating perplexity scores using model: {together_model}\")\n",
    "\n",
    "# Original abstract\n",
    "orig_perplexity, orig_zlib, orig_ratio = calculate_perplexity(\n",
    "    sample_paper.abstract, \"original\"\n",
    ")\n",
    "\n",
    "# Modified abstract\n",
    "mod_perplexity, mod_zlib, mod_ratio = calculate_perplexity(\n",
    "    sample_paper.gpt4_incorrect_abstract, \"modified\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf6f2a1",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "Let's analyze the results of our perplexity calculations and determine which abstract the model considers more likely to be the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4417cad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which abstract has lower perplexity\n",
    "if orig_perplexity < mod_perplexity:\n",
    "    preferred = \"ORIGINAL\"\n",
    "    diff = mod_perplexity - orig_perplexity\n",
    "    diff_percent = (diff / mod_perplexity) * 100\n",
    "else:\n",
    "    preferred = \"MODIFIED\"\n",
    "    diff = orig_perplexity - mod_perplexity\n",
    "    diff_percent = (diff / orig_perplexity) * 100\n",
    "\n",
    "print(f\"\\n=== RESULTS ANALYSIS ===\")\n",
    "print(f\"Model preferred the {preferred} abstract\")\n",
    "print(f\"Perplexity difference: {diff:.6f} ({diff_percent:.2f}% lower)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639c6279",
   "metadata": {},
   "source": [
    "## Storing the Results\n",
    "\n",
    "Finally, let's store these perplexity scores in the database for future analysis. This is the same process that happens in the `score_rag_perplexity_for_paper` function in the pipeline module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d2ed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to store perplexity event\n",
    "def create_perplexity_event(abstract_text, abstract_source, perplexity, zlib_size, zlib_ratio, full_prompt):\n",
    "    # Create metadata dictionary\n",
    "    metadata = {\n",
    "        \"method\": \"rag\",\n",
    "        \"rag_size_chars\": len(rag_context),\n",
    "        \"abstract_source\": abstract_source,\n",
    "        \"chunks_retrieved\": chunks_retrieved,\n",
    "        \"demo_notebook\": True\n",
    "    }\n",
    "    \n",
    "    # Create perplexity event\n",
    "    evt = PerplexityScoreEvent(\n",
    "        research_paper_id=sample_paper.id,\n",
    "        model=together_model,\n",
    "        abstract_text=abstract_text,\n",
    "        abstract_source=abstract_source,\n",
    "        prompt_template_name=\"rag_demo\",\n",
    "        full_prompt=full_prompt,\n",
    "        zlib_compression_size=zlib_size,\n",
    "        perplexity_score=perplexity,\n",
    "        zlib_perplexity_ratio=zlib_ratio,\n",
    "        evaluation_datetime=datetime.datetime.utcnow(),\n",
    "        additional_metadata=metadata\n",
    "    )\n",
    "    \n",
    "    session.add(evt)\n",
    "    return evt\n",
    "\n",
    "# Store perplexity events (commented out to prevent actual database writes)\n",
    "# Store for original abstract\n",
    "full_prompt_orig = assemble_test_abstract_prompt(rag_context.strip(), sample_paper.abstract.strip())\n",
    "# evt_orig = create_perplexity_event(\n",
    "#     sample_paper.abstract, \"original\", \n",
    "#     orig_perplexity, orig_zlib, orig_ratio, full_prompt_orig\n",
    "# )\n",
    "\n",
    "# Store for modified abstract\n",
    "full_prompt_mod = assemble_test_abstract_prompt(rag_context.strip(), sample_paper.gpt4_incorrect_abstract.strip())\n",
    "# evt_mod = create_perplexity_event(\n",
    "#     sample_paper.gpt4_incorrect_abstract, \"gpt4\", \n",
    "#     mod_perplexity, mod_zlib, mod_ratio, full_prompt_mod\n",
    "# )\n",
    "\n",
    "# session.commit()\n",
    "# print(\"Stored perplexity events in database\")\n",
    "print(\"Database storage commented out to prevent actual writes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072f6758",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated the complete RAG pipeline for perplexity scoring of research paper abstracts. We've seen how to:\n",
    "\n",
    "1. Create masked abstracts to identify differences between original and modified versions\n",
    "2. Generate service queries based on these masked abstracts\n",
    "3. Retrieve and filter relevant papers\n",
    "4. Generate a RAG context from these papers\n",
    "5. Calculate perplexity scores for both abstracts\n",
    "6. Analyze the results to determine which abstract is more likely to be original\n",
    "\n",
    "This approach demonstrates how RAG can be used to enhance the model's ability to distinguish between real and fake scientific content by grounding its evaluation in relevant literature.\n",
    "\n",
    "Such techniques can be valuable for detecting misinformation in scientific publications, helping to maintain the integrity of the scientific literature."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
